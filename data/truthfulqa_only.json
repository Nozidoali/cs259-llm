{
    "method": "rmoe",
    "base_model": "meta-llama/Llama-3.2-1B-Instruct",
    "datasets": ["truthfulqa"],
    "expert_training": {
      "max_length": 512,
      "num_epochs": 100,
      "batch_size": 1,
      "gradient_accumulation_steps": 4,
      "learning_rate": 1e-5,
      "weight_decay": 0.01,
      "eval_split": 0.2,
      "qmsum_max_new_tokens": 200,
      "temperature": 0.5,
      "l2_regularization": 1e-4
    },
    "seed": 42
  }