{
  "method": "rmoe",
  "base_model": "Qwen/Qwen2-0.5B-Instruct",
  "datasets": ["truthfulqa", "qmsum"],
  "seed": 42,
  "comment": "Full RMoE pipeline configuration for Qwen2-0.5B-Instruct",
  
  "expert_training": {
    "comment": "Train one expert per dataset",
    "num_epochs": 3,
    "batch_size": 2,
    "gradient_accumulation_steps": 8,
    "learning_rate": 5e-5,
    "weight_decay": 0.01,
    "max_length": 512,
    "l2_regularization": 1e-5,
    "max_grad_norm": 1.0,
    "disable_eval_split": false,
    "eval_split": 0.15,
    "qmsum_max_new_tokens": 200,
    "temperature": 0.7,
    "warmup_steps": 100,
    "save_steps": 500,
    "logging_steps": 50,
    "fp16": true
  },
  
  "gating": {
    "comment": "Gating network WITHOUT bias (required for Qwen2MoE compatibility)",
    "hidden_dims": [512, 256],
    "dropout": 0.1,
    "learning_rate": 1e-4,
    "batch_size": 32,
    "num_epochs": 10,
    "weight_decay": 0.01,
    "train_split": 0.7,
    "val_split": 0.15,
    "test_split": 0.15,
    "early_stopping_patience": 3,
    "min_delta": 0.001
  },
  
  "merge": {
    "comment": "Merge experts into MoE model using gating network",
    "routing_mode": "select_one",
    "comment_routing_mode": "Options: 'weighted_sum' (soft routing) or 'select_one' (hard routing)",
    "target_architecture": "qwen2moe",
    "comment_target_architecture": "Options: 'auto' (qwen3moe by default, qwen2moe if use_shared_expert=true), 'qwen2moe', 'qwen3moe'",
    "use_shared_expert": true,
    "comment_use_shared_expert": "Set to true to use original base model as shared expert (enables Qwen2MoE format)",
    "shared_expert_path": null,
    "comment_shared_expert_path": "null = use base_model as shared expert, or specify path to different model"
  },
  
  "full_finetune": {
    "comment": "Optional: Fine-tune the merged MoE model end-to-end",
    "enabled": false,
    "num_epochs": 2,
    "batch_size": 1,
    "gradient_accumulation_steps": 8,
    "learning_rate": 2e-5,
    "weight_decay": 0.01,
    "max_length": 512,
    "eval_split": 0.2,
    "warmup_steps": 50,
    "fp16": true
  },
  
  "conversion": {
    "comment": "Convert merged model to GGUF format for llama.cpp",
    "output_type": "f16",
    "comment_output_type": "Options: f16, f32, q4_0, q4_1, q5_0, q5_1, q8_0",
    "verify_conversion": true
  },
  
  "quantize": "q8_0",
  "comment_quantize": "Quantization type for final GGUF model. Options: Q4_0, Q4_K_M, Q5_K_M, Q8_0, etc.",
  
  "gguf_output": null,
  "comment_gguf_output": "Custom output path for GGUF file (null = auto-generate)",
  
  "paths": {
    "comment": "Optional: Override default paths",
    "work_dir": "./workspace",
    "experts_dir": null,
    "gating_dir": null,
    "merged_dir": null,
    "gguf_dir": null
  },
  
  "advanced": {
    "comment": "Advanced options for debugging and optimization",
    "device": "auto",
    "comment_device": "Options: 'auto', 'cuda', 'cpu', 'mps'",
    "gradient_checkpointing": false,
    "mixed_precision": "fp16",
    "dataloader_num_workers": 4,
    "pin_memory": true,
    "compile_model": false
  }
}

